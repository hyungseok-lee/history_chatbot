{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyungseok/anaconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gradio\n",
    "import tiktoken, os\n",
    "\n",
    "# from streamlit_chat import message\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.callbacks import get_openai_callback\n",
    "from langchain.memory import StreamlitChatMessageHistory\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(data_dir):\n",
    "    doc_list = []\n",
    "    pdf_list = sorted(os.listdir(data_dir))\n",
    "    for idx, pdf_name in enumerate(pdf_list):\n",
    "        if pdf_name.endswith(\".pdf\"):\n",
    "            loader = PyPDFLoader(os.path.join(data_dir, pdf_name))\n",
    "            documents = loader.load_and_split()\n",
    "            doc_list.extend(documents)\n",
    "            print(f\"{idx+1}/{len(pdf_list)} {pdf_name} loaded\")\n",
    "    print(f\"Total {idx+1} documents loaded\")\n",
    "    return doc_list\n",
    "\n",
    "def tiktoken_len(text):\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "\n",
    "\n",
    "def get_text_chunks(text):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=900,\n",
    "        chunk_overlap=100,\n",
    "        length_function=tiktoken_len\n",
    "    )\n",
    "    chunks = text_splitter.split_documents(text)\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "                                        model_name=\"jhgan/ko-sroberta-multitask\",\n",
    "                                        model_kwargs={'device': 'cpu'},\n",
    "                                        encode_kwargs={'normalize_embeddings': True}\n",
    "                                        )  \n",
    "    vectordb = FAISS.from_documents(text_chunks, embeddings)\n",
    "    return vectordb\n",
    "\n",
    "\n",
    "def get_conversation_chain(vetorestore,openai_api_key):\n",
    "    llm = ChatOpenAI(openai_api_key=openai_api_key, model_name = 'gpt-3.5-turbo-1106', temperature=0)\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm, \n",
    "            chain_type=\"stuff\", \n",
    "            retriever=vetorestore.as_retriever(search_type = 'mmr', vervose = True), \n",
    "            memory=ConversationBufferMemory(memory_key='chat_history', return_messages=True, output_key='answer'),\n",
    "            get_chat_history=lambda h: h,\n",
    "            return_source_documents=True,\n",
    "            verbose = True\n",
    "        )\n",
    "\n",
    "    return conversation_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/22 5-2-1-1(1지도서).pdf loaded\n",
      "2/22 5-2-1-1(2지도서).pdf loaded\n",
      "3/22 5-2-1-1(3지도서).pdf loaded\n",
      "4/22 5-2-1-1(4지도서).pdf loaded\n",
      "5/22 5-2-1-1(5지도서).pdf loaded\n",
      "6/22 5-2-1-1(6지도서).pdf loaded\n",
      "7/22 5-2-1-1(7지도서).pdf loaded\n",
      "8/22 5-2-1-2(10지도서).pdf loaded\n",
      "9/22 5-2-1-2(11지도서).pdf loaded\n",
      "10/22 5-2-1-2(12지도서).pdf loaded\n",
      "11/22 5-2-1-2(13지도서).pdf loaded\n",
      "12/22 5-2-1-2(14지도서).pdf loaded\n",
      "13/22 5-2-1-2(8지도서).pdf loaded\n",
      "14/22 5-2-1-2(9지도서).pdf loaded\n",
      "15/22 5-2-1-3(15지도서).pdf loaded\n",
      "16/22 5-2-1-3(16지도서).pdf loaded\n",
      "17/22 5-2-1-3(17지도서).pdf loaded\n",
      "18/22 5-2-1-3(18지도서).pdf loaded\n",
      "19/22 5-2-1-3(19-20지도서).pdf loaded\n",
      "20/22 5-2-1-3(21지도서).pdf loaded\n",
      "21/22 5-2-1-3(22지도서).pdf loaded\n",
      "22/22 5-2-1-3(23-24지도서).pdf loaded\n",
      "Total 22 documents loaded\n"
     ]
    }
   ],
   "source": [
    "openai_api_key = \"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\"\n",
    "data_dir = \"data/\" \n",
    "\n",
    "doc_list = load_and_process_data(data_dir)\n",
    "text_chunks = get_text_chunks(doc_list)\n",
    "vetorestore = get_vectorstore(text_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = get_conversation_chain(vetorestore, openai_api_key) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def predict(input, history):\n",
    "    history.append({\"role\": \"user\", \"content\": input})\n",
    "\n",
    "    gpt_response = client.chat.completions.create(\n",
    "        model=model_id,\n",
    "        messages=history\n",
    "    )\n",
    "    response = gpt_response.choices[0].message.content\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    messages = [(history[i][\"content\"], history[i+1][\"content\"]) for i in range(1, len(history), 2)]\n",
    "    return messages, history\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    chatbot = gr.Chatbot(label=\"ChatBot\")\n",
    "    state = gr.State([{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"ChatSNS is a chatbot really friendly and concise.\"\n",
    "    }])\n",
    "    with gr.Row():\n",
    "        txt = gr.Textbox(show_label=False, placeholder=\"챗봇에게 아무거나 물어보세요\")\n",
    "    txt.submit(predict, [txt, state], [chatbot, state])\n",
    "\n",
    "demo.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
